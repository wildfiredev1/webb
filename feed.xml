<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator>
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2022-03-16T20:24:00+05:30</updated>
  <id>http://localhost:4000/</id>

  
    <title type="html">Devashish Pandit</title>
  

  
    <subtitle>Howdy! This is Dev, welcome to my portfolio. Here you'll find a summary of what I've been up to.</subtitle>
  

  

  
  
    <entry>
      <title type="html">Project 3</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 3" />
      <published>2022-03-15T00:00:00+05:30</published>
      <updated>2022-03-15T00:00:00+05:30</updated>
      <id>http://localhost:4000/project-3</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Hey there! Certainly glad you could join us.&lt;/p&gt;

&lt;p&gt;In this notebook we’ll see how to create a convolutional neural network step-by-step with PyTorch[1]. We’ll build this classifier on the following dataset - &lt;a href=&quot;https://www.kaggle.com/alxmamaev/flowers-recognition&quot;&gt;“https://www.kaggle.com/alxmamaev/flowers-recognition”&lt;/a&gt;. [2]&lt;/p&gt;

&lt;p&gt;We’ll start by taking the tutorial provided &lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py&quot;&gt;here&lt;/a&gt; as a reference and build on top of it to improve performance. Let’s get started!&lt;/p&gt;

&lt;h3 id=&quot;understanding-the-dataset&quot;&gt;Understanding the dataset&lt;/h3&gt;

&lt;p&gt;Before going into details about the classifiers let’s take a moment to look at the dataset. &lt;br /&gt;
The dataset consists of 4242 images of flowers, collected from sources such as data flicr, google images, yandex images.&lt;br /&gt;
The images in the dataset are divided into five classes: daisy, tulip, rose, sunflower and dandelion. The number of instances for each category is as shown - 
&lt;img src=&quot;img/portfolio/flowers_data.png&quot; class=&quot;center&quot; /&gt;&lt;br /&gt;
The images are not extremely high resolution and neither do they all have the same proportions. It will be our task to make sure they are appropriatelly formatted and augmented before they are fed into the network.&lt;/p&gt;

&lt;h4 id=&quot;making-the-necessary-imports&quot;&gt;Making the necessary imports&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import shutil
import matplotlib.pyplot as plt
import numpy as np
import os
import torch
import torchvision
import torchvision.transforms as transforms

from scikitplot.metrics import confusion_matrix
import itertools

import yaml
import h5py

import seaborn as sns
sns.set()
import time

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;configuring-to-use-gpu&quot;&gt;Configuring to use GPU&lt;/h4&gt;
&lt;p&gt;We’ll be training this classifier on kaggle using GPU as accelerator. In case you don’t have GPU access, it will work just fine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
# Assuming that we are on a CUDA machine, this should print a CUDA device:
print(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For ensuring reproducibility, it is good practice to set random seed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
from numpy.random import seed
seed(42)
torch.manual_seed(42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s see an example of what the images look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
from PIL import Image
im = Image.open('../input/flowers-recognition/flowers/daisy/100080576_f52e8ee070_n.jpg')

print(im.size)
print(type(im.size))

w, h = im.size
print('width: ', w)
print('height:', h)
display(im)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_8_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;making-train-and-test-sets&quot;&gt;Making train and test sets&lt;/h3&gt;
&lt;p&gt;The dataset consists of images distributed in 5 folders/classes but there are no separate folders for training and testing data. This structure is not suitable for our purposes. Let’s redistribute the images into a train and a test set.&lt;br /&gt;
The following code snippet copies the data from input to output directory in the kaggle environment. If you are not using kaggle, this is not necesseary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
train_dir = './train'
shutil.copytree('../input/flowers-recognition/flowers',train_dir)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We first specify a directory to hold our test images and set the ratio of test images to be 33% of the total images. We want these images to be randomly selected such that the test set is a representative of the overall dataset in terms of number of images.&lt;br /&gt;
We achieve this by first finding out the total number of images in folder/class and then transferring 33% of each of them into the test folder, maintaing the directory structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
val_dir = './test/'
train_ratio = 0.77
_, dirs, _ = next(os.walk(train_dir))
images_per_class = np.zeros(5) # 5 is number of classes
for i in range(len(images_per_class)):
    path = os.path.join(train_dir,dirs[i])
    files = np.asarray(os.listdir(path))
    images_per_class[i] = len(files)
    
val_counter = np.round(images_per_class * (1-train_ratio))

# transfer files
for i in range(len(images_per_class)):
    source_path = os.path.join(train_dir, dirs[i])
    dest_path = os.path.join(val_dir, dirs[i])
    if not os.path.exists(dest_path):
        os.makedirs(dest_path)
    files = np.asarray(os.listdir(source_path))
    for j in range(int(val_counter[i])):
        dst = os.path.join(dest_path, files[j])
        src = os.path.join(source_path, files[j])
        shutil.move(src,dst)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, let’s transform these images in a way that is suitable for our purposes. We want each image to be of size 256 x 256 pixels and cropped with respect to the center.&lt;br /&gt;
It is good practice to augment the images before serving them into the network. It ensures generalization and may prevent overfitting. The augmentation we will apply are - Random Rotation (rotate images randomly by 25 degrees), Random Horizontal Flip (flip images randomly along the vertical axis) and normalization. It is especially recommended to normalize the data before feeding it into the network.[3]&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
transform = transforms.Compose([
    transforms.Resize(256), # we'll work with images of size 256 x 256
    transforms.CenterCrop(256),
    transforms.RandomRotation(25), # randomly rotate images by 25 degrees
    transforms.RandomHorizontalFlip(), # randomly flip images horizontally
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5,0.5,0.5],
                         std=[0.5,0.5,0.5] ) #normalize images for efficient training
    ])
batch = 16
batch_size = 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PyTorch models take data in the form of data loaders. Let’s create train and test dataloaders with image augmentations applied.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
train_data = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)
train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=4)
test_data = torchvision.datasets.ImageFolder(root=val_dir, transform=transform)
test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following grid of images show some instances from the dataset post-normalization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
classes = ['daisy','dandelion','rose','sunflower','tulip']

from mpl_toolkits.axes_grid1 import ImageGrid

dataiter = iter(train_data_loader)


def plottable(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    return np.transpose(npimg, (1, 2, 0))

images, labels = dataiter.next()
imgs = [plottable(img) for img in images]

fig = plt.figure(figsize=(4,4))
grid = ImageGrid(fig, 111,  # similar to subplot(111)
                 nrows_ncols=(2,2),  # creates 2x2 grid of axes
                 axes_pad=0.5,  # pad between axes in inch.
                 )

for i in range(len(imgs)):
    ax = grid[i]
    im = imgs[i]
    label = labels[i]
    ax.grid(False)
    plt.axis('off')
    # Iterating over the grid returns the Axes.
    ax.set_title(classes[label])
    ax.imshow(im)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_19_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;building-our-first-model&quot;&gt;Building our first model&lt;/h3&gt;

&lt;p&gt;Alright! We’re set to build our first model. We’ll refer the pytorch tutorial mentioned above to build our first classifier.
Refer - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py&lt;/p&gt;

&lt;h4 id=&quot;defining-a-simple-neural-network&quot;&gt;Defining a simple neural network&lt;/h4&gt;
&lt;p&gt;We’ll use the pytorch library to define a simple convolutional neural network. CNN[6] is one of the most utilized techniques in computer vision. They have been used to achieve near human and in some cases - even superhuman performances in tasks such as image classification, object detection etc. in competitions such as ImageNet.[4][5]&lt;br /&gt;
Libraries such as PyTorch abstract away the complexities and make it really easy to define even large networks.&lt;br /&gt;
Our CNN consists of the following layers -&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;b&gt;Convolution Layers&lt;/b&gt; - Convolution layers are the basic building blocks of a CNN (hence the name). The job of this layer is to perform the convolution operation on the input data. In the convolution operation, we define a kernel or a feature detector with some kernel size, which goes over the entire input grid and see if a feature is present. The feature detector is a two-dimensional (2-D) array of weights, which represents part of the image. The filter is applied to a part of the image and and a dot product is calculated between the input pixels and the filter. The output of the convolution layer consists of similar dot products taken by moving the filter all over the input data, governed by a factor called &quot;stride&quot;.  This output is called the feature map, or the activation map.&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Max Pooling Layers&lt;/b&gt; - Multiple convolution operations can quickly result in the amount of intermediate data getting out of hand. This is bad both in terms of memory and also it affects the ability of CNN to learn efficiently. This requires dimensionality reduction. As the filter moves across the input data, the max pooling layer selects the pixel with the maximum value to send to the output array.&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Fully Connected Layers&lt;/b&gt; - As the name suggests these layers consists of neurons that are connected to each neuron in both the previous and the next layers. The main job of these layers is to propagate the learned information forward, to the output layer where the classification is made.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our initial network, we have 2 convolution layers both followed by ReLU (rectified linear unit) activation function and max pooling layers. These are followed by 2 fully connected layers again with relu activation and a final fully connected layer that serves as the classification layer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 18, 5)
        self.fc1 = nn.Linear(18 * 61 * 61, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
net.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 18, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=66978, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=5, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The details of each layer can be seen in the output of the previous cell.&lt;br /&gt;&lt;br /&gt;
Now we’ll define the loss function and the optimizer for our model. Cross Entropy can be defined as a measure of dissimilarity between two probability distributions for a given random variable. In our case we are looking at the difference between the prediction made and how far is it from the actual label.&lt;br /&gt;
The optimizer of choice here is Stochastic Gradient Descent. The task of SGD here is to determine how to modify the parameters of the network in order to reduce the loss incurred by the model on a batch of input images.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import torch.optim as optim
import torch.nn as nn

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;training-the-network&quot;&gt;Training the network&lt;/h4&gt;
&lt;p&gt;Now we’re all set to commence training!&lt;br /&gt;
We’ll train this network for 45 epochs with the above mentioned hyperparameters and monitor the training.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
from tqdm import tqdm
start_time = int(time.time())
for epoch in range(45):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in tqdm(enumerate(train_data_loader, 0)):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 200 == 199:    # print every 200 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0
            
end_time = int(time.time())
print('Finished Training')
fl1_time = end_time-start_time
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;205it [00:10, 40.72it/s]

[1,   200] loss: 0.157


408it [00:14, 49.96it/s]

[1,   400] loss: 0.142


603it [00:18, 44.89it/s]

[1,   600] loss: 0.135


808it [00:23, 54.36it/s]

[1,   800] loss: 0.125


831it [00:23, 35.17it/s]
205it [00:04, 53.52it/s]

[2,   200] loss: 0.120


404it [00:09, 46.99it/s]

[2,   400] loss: 0.114


.
.
.
.

805it [00:17, 44.27it/s]

[44,   800] loss: 0.030


831it [00:18, 45.43it/s]
207it [00:04, 48.44it/s]

[45,   200] loss: 0.027


408it [00:08, 50.81it/s]

[45,   400] loss: 0.028


608it [00:12, 48.31it/s]

[45,   600] loss: 0.032


803it [00:16, 45.05it/s]

[45,   800] loss: 0.025


831it [00:17, 47.47it/s]

Finished Training
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
# Saving the model
PATH = './fl1.pth'
torch.save(net.state_dict(), PATH)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s see how we did. We’ll assess our models’s performance on the test set (with images our model hasn’t seen yet).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
correct = 0
total = 0
# since we're not training, we don't need to calculate the gradients for our outputs
with torch.no_grad():
    for data in test_data_loader:
        inputs, labels = data[0].to(device), data[1].to(device)
        # calculate outputs by running images through the network
        outputs = net(inputs)
        # the class with the highest energy is what we choose as prediction
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the test images: {100 * correct // total} %')
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Accuracy of the network on the test images: 68 %
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not bad for our first try! Now let’s see how our model did on individual classes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
# prepare to count predictions for each class
correct_pred = {classname: 0 for classname in classes}
total_pred = {classname: 0 for classname in classes}

# again no gradients needed
with torch.no_grad():
    for data in test_data_loader:
        inputs, labels = data[0].to(device), data[1].to(device)
        outputs = net(inputs)
        _, predictions = torch.max(outputs, 1)
        # collect the correct predictions for each class
        for label, prediction in zip(labels, predictions):
            if label == prediction:
                correct_pred[classes[label]] += 1
            total_pred[classes[label]] += 1

# print accuracy for each class
for classname, correct_count in correct_pred.items():
    accuracy = 100 * float(correct_count) / total_pred[classname]
    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Accuracy for class: daisy is 65.9 %
Accuracy for class: dandelion is 79.3 %
Accuracy for class: rose  is 60.6 %
Accuracy for class: sunflower is 73.4 %
Accuracy for class: tulip is 54.4 %
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s not bad! Let us see now, how we can improve upon these results. Moving forward we will be more concerned with the average accuracy of our model instead of it’s performance on individual classes.&lt;/p&gt;

&lt;h3 id=&quot;making-improvements---classifier-2&quot;&gt;Making Improvements - Classifier #2&lt;/h3&gt;

&lt;p&gt;In this instance, we’ll increase the number of layers in our model and make it deeper. We’ll also change the activation function to “leaky relu”[7] which is known to solve the problem of vanishing gradients present in “relu”.&lt;br /&gt;
The updated model will have 3 convolutional layers with max pooling and 4 fully connected layers all with leaky relu activation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
class Net1(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 12, 5)
        self.conv3 = nn.Conv2d(12,18, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(18 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64,32)
        self.fc4 = nn.Linear(32, 5)

    def forward(self, x):
        x = self.pool(F.leaky_relu(self.conv1(x)))
        x = self.pool(F.leaky_relu(self.conv2(x)))
        x = self.pool(F.leaky_relu(self.conv3(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = F.leaky_relu(self.fc3(x))
        x = self.fc4(x)
        return x


net1 = Net1()
net1.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net1(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
  (conv3): Conv2d(12, 18, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=14112, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=5, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The previous implementation was a tad bit inefficient and didn’t encourage reusability of code. So before starting training, let us define appropriate functions for training and testing the network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
def forwardPass(net,inputs, labels, train=False):
    if train:
        optimizer.zero_grad()
        net.zero_grad()
    total = 0
    correct = 0
    outputs = net(inputs)
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
    acc = correct/total
    loss = criterion(outputs, labels)

    if train:
        loss.backward()
        optimizer.step()

    return acc, loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this function does is - it takes in the model and the data as input along with the flag ‘train’ which determines whether we want to update the weights (train) or not (test). The function returns accuracy and loss incurred by the model on the current batch of data.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;While training this model, we’ll use the Adam optimizer[8] and we’ll use a learning rate of 0.001. We’ll also track the accuracy and losses by logging them in a file for analyzing later.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
MODEL_NAME = &quot;fl2&quot;
optimizer = optim.Adam(net1.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

def train(net,train_data_loader):
    EPOCHS = 45
    with open(&quot;flower_nets.log&quot;, &quot;a&quot;) as f:
        start_time = int(time.time())
        for epoch in range(EPOCHS):
            loss = 0
            acc = 0
            sz = 0
            for i, data in tqdm(enumerate(train_data_loader, 0)):
                images,labels = data[0].to(device), data[1].to(device)
                
                batch_acc,batch_loss = forwardPass(net,images,labels,train=True)
                loss += batch_loss
                acc += batch_acc

                if (i%250 ==0):
                    print(f&quot;Batch {i} Acc: {round(float(batch_acc),2)}  Loss: {round(float(batch_loss),4)}&quot;)
                batch_acc = 0
                batch_loss = 0
                sz += 1
                
            acc /= sz
            loss /= sz
            end_time = int(time.time())
            f.write(f&quot;{MODEL_NAME},{int(time.time())},{epoch},{round(float(acc),2)},{round(float(loss),4)}\n&quot;)
            print(f'Epoch {epoch} --------------- Accuracy : {round(float(acc),2)}, Loss : {round(float(loss),4)}')
        f.write(f&quot;{MODEL_NAME},time_taken,{int(time.time()) - start_time}\n&quot;)
        
start_time = int(time.time())
train(net1,train_data_loader)
end_time = int(time.time())
PATH = './fl2.pth'
torch.save(net1.state_dict(), PATH)
fl2_time = end_time - start_time
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;7it [00:00, 32.76it/s]

Batch 0 Acc: 0.0  Loss: 1.6611


256it [00:06, 34.12it/s]

Batch 250 Acc: 0.25  Loss: 1.0655


509it [00:11, 47.31it/s]

Batch 500 Acc: 0.25  Loss: 1.3603


760it [00:17, 48.53it/s]

Batch 750 Acc: 0.25  Loss: 1.4293


831it [00:18, 43.84it/s]


Epoch 0 --------------- Accuracy : 0.39, Loss : 1.3186


5it [00:00, 24.33it/s]

Batch 0 Acc: 1.0  Loss: 0.5471


261it [00:05, 51.46it/s]

Batch 250 Acc: 0.5  Loss: 0.9332


507it [00:11, 47.11it/s]

Batch 500 Acc: 1.0  Loss: 0.785


757it [00:16, 49.85it/s]

Batch 750 Acc: 0.5  Loss: 1.0572


831it [00:18, 46.16it/s]

Epoch 1 --------------- Accuracy : 0.52, Loss : 1.1354



5it [00:00, 24.44it/s]

Batch 0 Acc: 0.5  Loss: 0.6648


259it [00:06, 44.87it/s]

Batch 250 Acc: 0.25  Loss: 1.8352


509it [00:12, 39.20it/s]

Batch 500 Acc: 0.25  Loss: 1.3271


756it [00:17, 46.40it/s]

Batch 750 Acc: 0.75  Loss: 1.0471


831it [00:18, 43.81it/s]

Epoch 2 --------------- Accuracy : 0.58, Loss : 1.0235
.
.
.
.
   
6it [00:00, 29.93it/s]

Batch 0 Acc: 1.0  Loss: 0.1338


258it [00:05, 49.02it/s]

Batch 250 Acc: 1.0  Loss: 0.1549


507it [00:11, 41.07it/s]

Batch 500 Acc: 1.0  Loss: 0.0298


758it [00:17, 46.95it/s]

Batch 750 Acc: 0.75  Loss: 0.476


831it [00:19, 43.30it/s]

Epoch 39 --------------- Accuracy : 0.92, Loss : 0.2337



5it [00:00, 21.11it/s]

Batch 0 Acc: 1.0  Loss: 0.0162


256it [00:06, 40.34it/s]

Batch 250 Acc: 0.75  Loss: 1.2722


506it [00:13, 42.02it/s]

Batch 500 Acc: 1.0  Loss: 0.0866


755it [00:19, 40.51it/s]

Batch 750 Acc: 1.0  Loss: 0.023


831it [00:22, 37.41it/s]

Epoch 40 --------------- Accuracy : 0.91, Loss : 0.2551



6it [00:00, 26.08it/s]

Batch 0 Acc: 1.0  Loss: 0.0053


259it [00:05, 44.42it/s]

Batch 250 Acc: 1.0  Loss: 0.0054


507it [00:11, 39.09it/s]

Batch 500 Acc: 1.0  Loss: 0.0212


757it [00:16, 48.53it/s]

Batch 750 Acc: 1.0  Loss: 0.0669


831it [00:18, 44.94it/s]

Epoch 41 --------------- Accuracy : 0.93, Loss : 0.2185



7it [00:00, 32.41it/s]

Batch 0 Acc: 1.0  Loss: 0.011


256it [00:05, 42.07it/s]

Batch 250 Acc: 1.0  Loss: 0.4842


507it [00:11, 46.05it/s]

Batch 500 Acc: 0.75  Loss: 0.5261


761it [00:17, 46.89it/s]

Batch 750 Acc: 0.75  Loss: 0.3457


831it [00:19, 42.98it/s]

Epoch 42 --------------- Accuracy : 0.92, Loss : 0.2631



6it [00:00, 27.41it/s]

Batch 0 Acc: 1.0  Loss: 0.0979


256it [00:05, 45.22it/s]

Batch 250 Acc: 0.75  Loss: 0.4053


505it [00:11, 45.11it/s]

Batch 500 Acc: 1.0  Loss: 0.1584


759it [00:16, 45.81it/s]

Batch 750 Acc: 0.75  Loss: 1.1415


831it [00:18, 45.21it/s]

Epoch 43 --------------- Accuracy : 0.93, Loss : 0.221



6it [00:00, 30.66it/s]

Batch 0 Acc: 1.0  Loss: 0.0221


256it [00:05, 44.96it/s]

Batch 250 Acc: 1.0  Loss: 0.0268


506it [00:11, 45.59it/s]

Batch 500 Acc: 1.0  Loss: 0.0052


760it [00:17, 46.52it/s]

Batch 750 Acc: 1.0  Loss: 0.0878


831it [00:18, 44.48it/s]

Epoch 44 --------------- Accuracy : 0.93, Loss : 0.2264
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At the end of 45 epochs, our model’s training accuracy is over 90%. Not bad at all! Let’s see how our model’s accuracy and incurred loss progress over the epochs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import matplotlib.pyplot as plt
from matplotlib import style

def create_acc_loss_graph(model_name):
    logs = open(&quot;flower_nets.log&quot;, &quot;r&quot;).read().split(&quot;\n&quot;)
    epochs = []
    accuracies = []
    losses = []

    for line in logs:
        if model_name in line and 'time_taken' not in line:
            name, timestamp, epoch, acc, loss = line.split(&quot;,&quot;)
            epochs.append(int(epoch))
            accuracies.append(float(acc))
            losses.append(float(loss))


    fig = plt.figure(figsize=(15,10))

    plt.plot(epochs, accuracies, label=&quot;Accuracy&quot;)
    plt.plot(epochs, losses, label=&quot;Loss&quot;)
    plt.title(f&quot;Accuracy and Loss for model {model_name}&quot;)
    plt.legend()
    plt.savefig(f'./{model_name}.png')
    plt.show()

create_acc_loss_graph('fl2')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_40_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
# testing
@torch.no_grad()
def test(net,test_data_loader):
    batch_loss = 0
    batch_acc = 0
    sz = 0
    acc = 0
    loss = 0
    
    for i, data in tqdm(enumerate(test_data_loader, 0)):
        images,labels = data[0].to(device), data[1].to(device)
        batch_acc,batch_loss = forwardPass(net,images,labels,train=False)
        loss += batch_loss
        acc += batch_acc
        batch_acc = 0
        batch_loss = 0
        sz += 1

    acc /= sz
    loss /= sz
    return acc, loss

acc_fl2,loss_fl2 = test(net1,test_data_loader)
print(f'Test ------------------------------- Accuracy = {acc_fl2}, Loss = {loss_fl2}')
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;249it [00:04, 50.42it/s]

Test ------------------------------- Accuracy = 0.7008032128514057, Loss = 1.3295061588287354
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note : the decorator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.no_grad()&lt;/code&gt; indicates that this function will run without computing the gradients, which is what we want during testing.&lt;br /&gt;&lt;br /&gt;
The test accuracy of 70% although a little better than the previous model is way lower than our training accuracy. This is a clear sign of overfitting. The model has learnt to classify the training data too well, so much that it can’t generalize as well on data it hasn’t seen yet. Let’s see how we can take care of this.&lt;/p&gt;

&lt;h3 id=&quot;classifier-3&quot;&gt;Classifier #3&lt;/h3&gt;

&lt;p&gt;It is good practice to use a validation set to test the model at regular intervals. The model is not trained on validation set, so it doesn’t learn anything from it. And this way we can use techniques like early stopping or picking the weights that gave the best resut on validation set - to select the best model.&lt;/p&gt;

&lt;h4 id=&quot;making-the-validation-set&quot;&gt;Making the validation set&lt;/h4&gt;

&lt;p&gt;We’ll follow the same process as we made when making train and test sets. We’ll assign 10% of the training data as test set and move it to the validation directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
val_dir = './val/'
train_ratio = 0.90
_, dirs, _ = next(os.walk(train_dir))
images_per_class = np.zeros(5) # 5 is number of classes
for i in range(len(images_per_class)):
    path = os.path.join(train_dir,dirs[i])
    files = np.asarray(os.listdir(path))
    images_per_class[i] = len(files)
    
val_counter = np.round(images_per_class * (1-train_ratio))

# transfer files
for i in range(len(images_per_class)):
    source_path = os.path.join(train_dir, dirs[i])
    dest_path = os.path.join(val_dir, dirs[i])
    if not os.path.exists(dest_path):
        os.makedirs(dest_path)
    files = np.asarray(os.listdir(source_path))
    for j in range(int(val_counter[i])):
        dst = os.path.join(dest_path, files[j])
        src = os.path.join(source_path, files[j])
        shutil.move(src,dst)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
val_ds = torchvision.datasets.ImageFolder(root=val_dir, transform=transform)
train_ds = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)
val_data_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True,  num_workers=4)
train_data_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Various model architectures were tried before settling on this one. The experimentations were by no means exhaustive and the reader is encouraged to try various architectures, making the changes they see fit.&lt;br /&gt;
 In this model architecture, we are using 5 convolution layers each with leaky relu activation, followed by batch-normalization as a regularizing technique and max pooling. The model uses 3 fully connected layers, 2 of which are followed by leaky relu activation and dropout. Dropout is also a batch normalization technique where a pre-specified percentage of neurons are randomly turned off to regularize training.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import torch.nn as nn

class Net2(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3,64,5)
        self.pool = nn.MaxPool2d(2, 2)
        self.bnorm1 = nn.BatchNorm2d(64)
        self.dropout = nn.Dropout(p=0.3)
        self.conv2 = nn.Conv2d(64,128,3)
        self.bnorm2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128,256,3)
        self.bnorm3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256,512,3)
        self.bnorm4 = nn.BatchNorm2d(512)
        self.conv5 = nn.Conv2d(512,1024,3)
        self.bnorm5 = nn.BatchNorm2d(1024)
        self.fc1 = nn.Linear(1024 * 6 * 6, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc6 = nn.Linear(256,5)

    def forward(self, x):
        x = self.pool(self.bnorm1(F.leaky_relu(self.conv1(x))))
        x = self.pool(self.bnorm2(F.leaky_relu(self.conv2(x))))
        x = self.pool(self.bnorm3(F.leaky_relu(self.conv3(x))))
        x = self.pool(self.bnorm4(F.leaky_relu(self.conv4(x))))
        x = self.pool(self.bnorm5(F.leaky_relu(self.conv5(x))))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.dropout(F.leaky_relu(self.fc1(x)))
        x = self.dropout(F.leaky_relu(self.fc2(x)))
        x = self.fc6(x)
        return x


net2 = Net2()
net2.to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Net2(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout): Dropout(p=0.3, inplace=False)
  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
  (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
  (bnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))
  (bnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))
  (bnorm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=36864, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=256, bias=True)
  (fc6): Linear(in_features=256, out_features=5, bias=True)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this instance, we’ll select the model which performs best on validation set as the final model. This will increase the odds of selecting a model that is not very overfitted (some overfitting is always desired).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
from tqdm import tqdm
import torch.nn.functional as F
import torch.optim as optim

MODEL_NAME = &quot;fl3&quot;
optimizer = optim.Adam(net2.parameters(), lr=0.0005)
criterion = nn.CrossEntropyLoss()

def train(net,MODEL_NAME):
    EPOCHS = 50
    with open(&quot;flower_nets.log&quot;, &quot;a&quot;) as f:
        start_time = int(time.time())
        max_val_acc = 0
        for epoch in range(EPOCHS):
            loss = 0
            acc = 0
            sz = 0
            for i, data in tqdm(enumerate(train_data_loader, 0)):
                images,labels = data[0].to(device), data[1].to(device)
                
                batch_acc,batch_loss = forwardPass(net,images,labels,train=True)
                loss += batch_loss
                acc += batch_acc
                
                if (i%250 ==0):
                    print(f&quot;Batch {i} Acc: {round(float(batch_acc),2)}  Loss: {round(float(batch_loss),2)}&quot;)
                batch_acc = 0
                batch_loss = 0
                sz += 1
                
            acc /= sz
            loss /= sz
            f.write(f&quot;{MODEL_NAME}_train,{int(time.time())},{epoch},{round(float(acc),2)},{round(float(loss),2)}\n&quot;)
            print(f'Epoch {epoch} --------------- Train Accuracy : {round(float(acc),2)}, Train Loss : {round(float(loss),2)}')
            
            val_acc,val_loss = test(net,val_data_loader)
            f.write(f&quot;{MODEL_NAME}_val,{int(time.time())},{epoch},{round(float(val_acc),2)},{round(float(val_loss),2)}\n&quot;)
            print(f'Epoch {epoch} --------------- Validation Accuracy : {round(float(val_acc),2)}, Validation Loss : {round(float(val_loss),2)}')
            
            if(val_acc &amp;gt; max_val_acc):
                max_val_acc = val_acc
                PATH = './fl3.pth'
                torch.save(net.state_dict(), PATH)
                
        f.write(f&quot;{MODEL_NAME},time_taken,{int(time.time()) - start_time}\n&quot;)
        
start_time = int(time.time())
train(net2,MODEL_NAME)
end_time = int(time.time())
fl3_time = end_time - start_time
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5it [00:00, 13.94it/s]

Batch 0 Acc: 0.0  Loss: 1.67


258it [00:07, 36.34it/s]

Batch 250 Acc: 0.0  Loss: 3.13


506it [00:14, 37.16it/s]

Batch 500 Acc: 0.25  Loss: 1.17


748it [00:21, 35.44it/s]

Epoch 0 --------------- Train Accuracy : 0.31, Train Loss : 2.21



83it [00:01, 43.16it/s]


Epoch 0 --------------- Validation Accuracy : 0.35, Validation Loss : 1.55


5it [00:00, 22.07it/s]

Batch 0 Acc: 0.5  Loss: 1.32


256it [00:07, 37.00it/s]

Batch 250 Acc: 0.75  Loss: 1.29


507it [00:14, 36.62it/s]

Batch 500 Acc: 0.5  Loss: 1.83


748it [00:21, 34.39it/s]

Epoch 1 --------------- Train Accuracy : 0.41, Train Loss : 1.47



83it [00:01, 45.93it/s]


Epoch 1 --------------- Validation Accuracy : 0.41, Validation Loss : 1.4


4it [00:00, 18.91it/s]

Batch 0 Acc: 0.25  Loss: 1.24


256it [00:07, 34.40it/s]

Batch 250 Acc: 0.75  Loss: 0.78


504it [00:14, 32.38it/s]

Batch 500 Acc: 1.0  Loss: 0.92


748it [00:21, 34.68it/s]

Epoch 2 --------------- Train Accuracy : 0.44, Train Loss : 1.39



83it [00:01, 45.65it/s]


Epoch 2 --------------- Validation Accuracy : 0.45, Validation Loss : 1.35


4it [00:00, 19.37it/s]

Batch 0 Acc: 0.5  Loss: 1.22


255it [00:07, 35.80it/s]

Batch 250 Acc: 0.25  Loss: 1.32


507it [00:14, 36.26it/s]

Batch 500 Acc: 0.75  Loss: 1.52


748it [00:20, 35.69it/s]

Epoch 3 --------------- Train Accuracy : 0.45, Train Loss : 1.35



83it [00:02, 40.34it/s]


Epoch 3 --------------- Validation Accuracy : 0.46, Validation Loss : 1.41


4it [00:00, 17.68it/s]

Batch 0 Acc: 0.5  Loss: 2.11


256it [00:07, 36.02it/s]

Batch 250 Acc: 0.75  Loss: 1.23


507it [00:14, 34.22it/s]

Batch 500 Acc: 0.25  Loss: 1.1


748it [00:21, 35.47it/s]

Epoch 4 --------------- Train Accuracy : 0.47, Train Loss : 1.35



83it [00:01, 45.72it/s]


Epoch 4 --------------- Validation Accuracy : 0.51, Validation Loss : 1.36
.
.
.
.


6it [00:00, 22.58it/s]

Batch 0 Acc: 0.75  Loss: 0.55


258it [00:07, 35.37it/s]

Batch 250 Acc: 1.0  Loss: 0.41


506it [00:14, 36.63it/s]

Batch 500 Acc: 0.75  Loss: 0.33


748it [00:21, 35.08it/s]

Epoch 47 --------------- Train Accuracy : 0.82, Train Loss : 0.52



83it [00:01, 45.88it/s]

Epoch 47 --------------- Validation Accuracy : 0.71, Validation Loss : 0.89



5it [00:00, 23.38it/s]

Batch 0 Acc: 1.0  Loss: 0.08


258it [00:07, 35.73it/s]

Batch 250 Acc: 0.75  Loss: 0.55


506it [00:14, 36.80it/s]

Batch 500 Acc: 0.75  Loss: 0.35


748it [00:21, 34.75it/s]

Epoch 48 --------------- Train Accuracy : 0.83, Train Loss : 0.48



83it [00:01, 45.61it/s]


Epoch 48 --------------- Validation Accuracy : 0.7, Validation Loss : 0.85


5it [00:00, 19.92it/s]

Batch 0 Acc: 1.0  Loss: 0.35


257it [00:07, 36.01it/s]

Batch 250 Acc: 1.0  Loss: 0.22


504it [00:14, 19.06it/s]

Batch 500 Acc: 1.0  Loss: 0.28


748it [00:21, 34.40it/s]

Epoch 49 --------------- Train Accuracy : 0.82, Train Loss : 0.5



83it [00:01, 46.02it/s]

Epoch 49 --------------- Validation Accuracy : 0.71, Validation Loss : 1.05
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The training resulted in 82% train accuracy and 71% validation accuracy. Let us see if we can improve performance by resuming training with a lower learning rate.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
from tqdm import tqdm
import torch.nn.functional as F
import torch.optim as optim

MODEL_NAME = &quot;fl3&quot;
optimizer = optim.Adam(net2.parameters(), lr=0.00005)
criterion = nn.CrossEntropyLoss()

def train(net,MODEL_NAME):
    EPOCHS = 20
    with open(&quot;flower_nets.log&quot;, &quot;a&quot;) as f:
        start_time = int(time.time())
        max_val_acc = 0
        for epoch in range(50,50+EPOCHS):
            loss = 0
            acc = 0
            sz = 0
            for i, data in tqdm(enumerate(train_data_loader, 0)):
                images,labels = data[0].to(device), data[1].to(device)
                
                batch_acc,batch_loss = forwardPass(net,images,labels,train=True)
                loss += batch_loss
                acc += batch_acc
                
                if (i%250 ==0):
                    print(f&quot;Batch {i} Acc: {round(float(batch_acc),2)}  Loss: {round(float(batch_loss),2)}&quot;)
                batch_acc = 0
                batch_loss = 0
                sz += 1
                
            acc /= sz
            loss /= sz
            f.write(f&quot;{MODEL_NAME}_train,{int(time.time())},{epoch},{round(float(acc),2)},{round(float(loss),2)}\n&quot;)
            print(f'Epoch {epoch} --------------- Train Accuracy : {round(float(acc),2)}, Train Loss : {round(float(loss),2)}')
            
            val_acc,val_loss = test(net,val_data_loader)
            f.write(f&quot;{MODEL_NAME}_val,{int(time.time())},{epoch},{round(float(val_acc),2)},{round(float(val_loss),2)}\n&quot;)
            print(f'Epoch {epoch} --------------- Validation Accuracy : {round(float(val_acc),2)}, Validation Loss : {round(float(val_loss),2)}')
            
            if(val_acc &amp;gt; max_val_acc):
                max_val_acc = val_acc
                PATH = './fl3.pth'
                torch.save(net.state_dict(), PATH)
                
#         f.write(f&quot;{MODEL_NAME},time_taken,{int(time.time()) - start_time}\n&quot;)
        
start_time = int(time.time())
train(net2,MODEL_NAME)
end_time = int(time.time())
fl3_time = fl3_time + end_time - start_time
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4it [00:00, 17.73it/s]

Batch 0 Acc: 0.5  Loss: 1.09


254it [00:07, 19.60it/s]

Batch 250 Acc: 0.75  Loss: 0.52


508it [00:14, 37.23it/s]

Batch 500 Acc: 1.0  Loss: 0.16


748it [00:21, 34.64it/s]

Epoch 50 --------------- Train Accuracy : 0.86, Train Loss : 0.39



83it [00:01, 45.53it/s]


Epoch 50 --------------- Validation Accuracy : 0.74, Validation Loss : 1.0
.
.
.
.

Epoch 62 --------------- Validation Accuracy : 0.75, Validation Loss : 1.38


5it [00:00, 21.65it/s]

Batch 0 Acc: 1.0  Loss: 0.13


257it [00:07, 35.91it/s]

Batch 250 Acc: 1.0  Loss: 0.03


505it [00:14, 36.58it/s]

Batch 500 Acc: 1.0  Loss: 0.34


748it [00:21, 34.70it/s]

Epoch 63 --------------- Train Accuracy : 0.9, Train Loss : 0.25



83it [00:01, 45.11it/s]


Epoch 63 --------------- Validation Accuracy : 0.77, Validation Loss : 1.12


4it [00:00, 20.55it/s]

Batch 0 Acc: 1.0  Loss: 0.08


258it [00:07, 36.94it/s]

Batch 250 Acc: 0.75  Loss: 0.21


507it [00:14, 37.04it/s]

Batch 500 Acc: 1.0  Loss: 0.16


748it [00:20, 35.80it/s]

Epoch 64 --------------- Train Accuracy : 0.91, Train Loss : 0.26



83it [00:01, 44.88it/s]

Epoch 64 --------------- Validation Accuracy : 0.76, Validation Loss : 1.34



5it [00:00, 22.57it/s]

Batch 0 Acc: 1.0  Loss: 0.0


258it [00:07, 36.61it/s]

Batch 250 Acc: 1.0  Loss: 0.07


507it [00:14, 34.89it/s]

Batch 500 Acc: 1.0  Loss: 0.26


748it [00:21, 34.84it/s]

Epoch 65 --------------- Train Accuracy : 0.91, Train Loss : 0.25



83it [00:01, 45.40it/s]

Epoch 65 --------------- Validation Accuracy : 0.75, Validation Loss : 1.2



5it [00:00, 21.87it/s]

Batch 0 Acc: 0.5  Loss: 0.87


258it [00:07, 37.16it/s]

Batch 250 Acc: 0.75  Loss: 0.28


507it [00:14, 35.59it/s]

Batch 500 Acc: 1.0  Loss: 0.0


748it [00:21, 34.76it/s]

Epoch 66 --------------- Train Accuracy : 0.92, Train Loss : 0.24



83it [00:02, 40.09it/s]


Epoch 66 --------------- Validation Accuracy : 0.75, Validation Loss : 1.26


5it [00:00, 22.35it/s]

Batch 0 Acc: 1.0  Loss: 0.0


257it [00:07, 37.20it/s]

Batch 250 Acc: 1.0  Loss: 0.0


505it [00:14, 36.54it/s]

Batch 500 Acc: 1.0  Loss: 0.0


748it [00:20, 36.19it/s]

Epoch 67 --------------- Train Accuracy : 0.92, Train Loss : 0.23



83it [00:02, 31.79it/s]

Epoch 67 --------------- Validation Accuracy : 0.73, Validation Loss : 1.41



5it [00:00, 22.48it/s]

Batch 0 Acc: 1.0  Loss: 0.06


257it [00:07, 37.07it/s]

Batch 250 Acc: 1.0  Loss: 0.0


507it [00:14, 37.43it/s]

Batch 500 Acc: 0.75  Loss: 0.88


748it [00:21, 35.51it/s]

Epoch 68 --------------- Train Accuracy : 0.92, Train Loss : 0.23



83it [00:01, 45.89it/s]

Epoch 68 --------------- Validation Accuracy : 0.76, Validation Loss : 1.2



3it [00:00, 15.43it/s]

Batch 0 Acc: 1.0  Loss: 0.14


255it [00:07, 24.98it/s]

Batch 250 Acc: 1.0  Loss: 0.28


508it [00:14, 36.74it/s]

Batch 500 Acc: 0.5  Loss: 0.63


748it [00:21, 34.67it/s]

Epoch 69 --------------- Train Accuracy : 0.92, Train Loss : 0.23



83it [00:01, 45.37it/s]

Epoch 69 --------------- Validation Accuracy : 0.76, Validation Loss : 1.24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
def create_acc_loss_graph(model_name,kind='train'):
    logs = open(&quot;flower_nets.log&quot;, &quot;r&quot;).read().split(&quot;\n&quot;)
    epochs = []
    accuracies = []
    losses = []

    for line in logs:
        if f'{model_name}_{kind}' in line and 'time_taken' not in line:
            name, timestamp, epoch, acc, loss = line.split(&quot;,&quot;)
            epochs.append(int(epoch))
            accuracies.append(float(acc))
            losses.append(float(loss))


    fig = plt.figure(figsize=(15,10))

    plt.plot(epochs, accuracies, label=&quot;Accuracy&quot;)
    plt.plot(epochs, losses, label=&quot;Loss&quot;)
    plt.title(f&quot;{kind} Accuracy and Loss for model {model_name}&quot;)
    plt.legend()
    plt.savefig(f'./{model_name}_{kind}.png')
    plt.show()

create_acc_loss_graph('fl3','train')
create_acc_loss_graph('fl3','val')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_57_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_57_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
model = Net2()
model.to(device)
model.load_state_dict(torch.load('./fl3.pth'))
acc_fl3,loss_fl3 = test(model,test_data_loader)

print(acc_fl3,loss_fl3)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.7389558232931727 tensor(1.3537, device='cuda:0')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This classifier has performed better than the previous one by 4%!&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparing-performances&quot;&gt;Comparing Performances&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
fl1 = [acc_fl1,loss_fl1.cpu().detach().numpy().item()]
fl2 = [acc_fl2,loss_fl2.cpu().detach().numpy().item()]
fl3 = [acc_fl3,loss_fl3.cpu().detach().numpy().item()]

bar_width = 0.25
fig = plt.subplots(figsize =(12, 8))

br1 = np.arange(len(fl1))
br2 = [x + bar_width for x in br1]
br3 = [x + bar_width for x in br2]
 
# Make the plot
plt.bar(br1, fl1, color ='r', width = bar_width,
        edgecolor ='grey', label ='model 1')
plt.bar(br2, fl2, color ='g', width = bar_width,
        edgecolor ='grey', label =' model 2')
plt.bar(br3, fl3, color ='b', width = bar_width,
        edgecolor ='grey', label ='model 3')
 
# Adding Xticks
plt.xlabel('Model', fontweight ='bold', fontsize = 15)
plt.ylabel('Value', fontweight ='bold', fontsize = 15)
plt.xticks([r + bar_width for r in range(len(fl1))],
        ['acc','loss'])

plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_62_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
times = [fl1_time,fl2_time,fl3_time]
models = ['model 1','model 2','model 3']
# Make the plot
plt.bar(models,times,width = 0.4)

# Adding Xticks
plt.xlabel('Model', fontweight ='bold', fontsize = 15)
plt.ylabel('Training Time', fontweight ='bold', fontsize = 15)
plt.title(&quot;Comparison of training times&quot;,fontsize = 18)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;img/portfolio/dev-flowers-pytorch_files/dev-flowers-pytorch_63_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this notebook we saw 3 CNN architectures built with progressive changes in layers, activations and optimizers. We compared their training times and performances over the same test set and charted them. Model performances can be further improved by more intensive experimentation and the reader is encouraged to try and build a model with better performance, using the steps highlighted in this notebook as a reference.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;[1]Paszke, A. et al., 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32. Curran Associates, Inc., pp. 8024–8035. Available at: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
&lt;br /&gt;
[2] Kaggle.com. 2022. Flowers Recognition. [online] Available at: &lt;a href=&quot;https://www.kaggle.com/alxmamaev/flowers-recognition&quot;&gt;https://www.kaggle.com/alxmamaev/flowers-recognition&lt;/a&gt; [Accessed 16 March 2022].
&lt;br /&gt;
[3]Huang, L. et al. (2020) “Normalization Techniques in Training DNNs: Methodology, Analysis and Application”, CoRR, abs/2009.12836. Available at: https://arxiv.org/abs/2009.12836.
&lt;br /&gt;
[4]Sultana, F., Sufian, A. en Dutta, P. (2019) “Advancements in Image Classification using Convolutional Neural Network”, CoRR, abs/1905.03288. Available at: http://arxiv.org/abs/1905.03288.
&lt;br /&gt;
[5]Deng, J. et al., 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255.
&lt;br /&gt;
[6]O’Shea, K. en Nash, R. (2015) “An Introduction to Convolutional Neural Networks”, CoRR, abs/1511.08458. Available at: http://arxiv.org/abs/1511.08458.
&lt;br /&gt;
[7]Xu, B. et al. (2015) “Empirical Evaluation of Rectified Activations in Convolutional Network”, CoRR, abs/1505.00853. Available at: http://arxiv.org/abs/1505.00853.
&lt;br /&gt;
[8]Kingma, D. P. en Ba, J. (2015) “Adam: A Method for Stochastic Optimization”, in Bengio, Y. en LeCun, Y. (reds) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Available at: http://arxiv.org/abs/1412.6980.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Deep Learning" />
      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 2</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 2" />
      <published>2022-02-21T00:00:00+05:30</published>
      <updated>2022-02-21T00:00:00+05:30</updated>
      <id>http://localhost:4000/project-2</id>
      <content type="html" xml:base="http://localhost:4000/"></content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Data Science" />
      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 1</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 1" />
      <published>2022-02-20T00:00:00+05:30</published>
      <updated>2022-02-20T00:00:00+05:30</updated>
      <id>http://localhost:4000/project-1</id>
      <content type="html" xml:base="http://localhost:4000/"></content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Data Science" />
      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
</feed>
